# Evaluation Metrics for Recommendation Systems
* This repo covers metrics commonly used for recommendation and ranking systems in data science, machine learning, and mathematics.


# Overview
* Recommendations and ranking systems share the goal of returning a list of items sorted by relevance.
* To evaluate recommendations, you must have predictions as user-item pairs, the binary or graded relevance score as the ground truth, and choose the K parameter. 
* The K parameter sets the cutoff point for the top recommendations you evaluate.
* You can use predictive metrics like accuracy or Precision at K or ranking metrics like NDCG, MRR, or MAP at K.  
* To go beyond accuracy, you can use behavioral metrics like serendipity, novelty, or diversity of recommendations. 
* To measure the actual impact, you can track business metrics like sales, click-through rates, or conversions during online evaluations.

# Evaluation Principles

## Input Data
* You need model predictions and ground truths to evaluate recommendation or ranking quality.
* Predictions are a list of ranked items generated by a model. 
* The ground truth is the actual relevance label or score.
 
 
## Relevance
* Relevance reflects the quality of the individual item in a list. 
* It can be…..
  * **Binary score** - (e.g., based on the user action – like clicking, watching, or buying a recommended item) or
  * **Graded score** - (e.g. rating from 1 to 5)
 
## Top-K Parameter
* K parameter is an evaluation cutoff point. 
* It represents the number of top-ranked items to evaluate. 
* For example, you can focus on the quality of top-10 recommendations.


# Metric Types
* Predictive metrics evaluate how accurate a model’s predictions are. 
* Ranking metrics show how well the items in the list are ordered. 
* Behavioral metrics reflect important system properties like diversity of recommendations.

## To evaluate a recommendation or ranking system, you need:
1. **Model predictions** - These include the ranked list of user-item pairs. The complete dataset also contains features that describe users or items. You’ll need them for some of the metrics. 

2. **Ground truth** - You need to know the actual user-item relevance to evaluate the quality of predictions. This might be a binary or graded relevance score. It is often based on the user interactions, such as clicks and conversions. 

3. **The K** - You need to pick the number of the top recommendations to consider. This puts a constraint on evaluations: you will disregard anything that happens after this cutoff point. 

## We can group the recommender or ranking quality metrics into 3 categories:
1. **Predictive metrics**
   * These reflect the “correctness” of recommendations and show how well the system finds relevant items. 

2. **Ranking metrics**
   * These reflect the ranking quality: how well the system can sort the items from more relevant to less relevant.

3. **Behavioral metrics**
   * These metrics reflect specific properties of the system, such as how diverse or novel the recommendations are.
 
## No single metric can capture all aspects of system performance at once. 
  * You can combine several offline metrics for a comprehensive evaluation. 
  * Additionally, you can incorporate online business metrics, user feedback, and subjective evaluations for a more holistic assessment.


# Predictive Metrics

## 1. Precision at K
* This measures the proportion of relevant items among the top K items.

![image](https://github.com/user-attachments/assets/77973096-8ba1-4ac1-bb42-1a420eefccf4)

## 2. Recall at K
* Measures the coverage of relevant items in the top K.

![image](https://github.com/user-attachments/assets/1c19558f-14e7-45ec-8e12-ee7fe73e136f)

## 3. F-score
* The F Beta score is a metric that balances Precision and Recall.
* F Beta score at K combines Precision and Recall metrics into a single value to provide a balanced assessment. 
  * The Beta parameter allows adjusting the importance of Recall relative to Precision. 
* The F Beta score is a good metric when you care about both properties: correctness of predictions and ability to cover as many relevant items as possible with the top-K. 
  * The Beta parameter allows you to customize the priorities.
 
![image](https://github.com/user-attachments/assets/c318e1e1-5127-41ec-8d04-d832a812d1f3)


### Can you limit the ranking evaluation to predictive metrics? 
* Metrics like Precision and Recall are very interpretable and easy to explain to non-technical users.
* They successfully capture the correctness of predictions. 
* Both make them very useful. However, they have limitations. 

**Precision and Recall depend heavily on the total number of relevant items.**
  * Because of this, it might be challenging to compare the performance across different lists.

**In addition, metrics like Precision and Recall are not rank-aware.** 
  * They are indifferent to the position of relevant items inside the top K. 
 
**Consider two lists that both have 5 out of 10 matches.**
  * In the first list, the relevant items are at the very top. 
  * In the second, they are at the very bottom. 
  * The Precision will be the same (50%) as long as the total number of relevant items is.

![image](https://github.com/user-attachments/assets/b01bbe51-5c2e-4442-9955-86538b02b1f1)


# Ranking Metrics
* Ranking metrics help assess the ability to order the items based on their relevance to the user or query. 
* In an ideal system, all relevant items should appear ahead of the less relevant ones. 
* Ranking metrics help measure how far you are from this.
 
## 1. MRR - Mean Reciprocal Rank
* Calculates the average of the reciprocal ranks of the first relevant item.
* To calculate MRR, you take the reciprocal of the rank of the first relevant item and average this value across all queries or users. 
* For example, if the first relevant item appears in the second position, this list's RR (Reciprocal Rank) is 1/2. If the first relevant item takes the third place, then the RR equals 1/3, and so on.

![image](https://github.com/user-attachments/assets/e4930fc6-e3da-47cc-8f85-3d818a30ef3a)

* MRR is an easy-to-understand and intuitive metric. 
  * It is beneficial when the top-ranked item matters: for example, you expect the search engine to return a relevant first result.
* However, the **limitation is that MRR solely focuses on the first relevant item and disregards all the rest.** In case you care about overall ranking, you might need additional metrics.

## 2. MAP - Mean Average Precision
* Measures the average Precision across different Recall levels for a ranked list.
* **Mean Average Precision (MAP) at K** evaluates the average Precision at all relevant ranks within the list of top K recommendations. 
  * This helps get a comprehensive measure of recommendation system performance, accounting for the quality of the ranking.
  * To compute MAP, you first need to calculate the Average Precision (AP) for each list: an average of Precision values at all positions in K with relevant recommendations.
 
![image](https://github.com/user-attachments/assets/5ae9a16a-ebaf-4fb2-991d-74de60123795)

* **MAP helps address the limitations of “classic” Prediction and Recall: it evaluates both the correctness of recommendations and how well the system can sort the relevant items inside the list.**
* Due to the underlying formula, MAP heavily rewards correct recommendations at the top of the list. Otherwise, you will factor the errors at the top in every consecutive Precision computation. 
* MAP is a valuable metric when it is important to get the top predictions right, like in information retrieval. 
* As a downside, this metric might be hard to communicate and does not have an immediate intuitive explanation.

## 3. Hit Rate
* Measures the share of users that get at least one relevant recommendation.
* **Hit Rate at K** calculates the share of users for which at least one relevant item is present in the K. This metric is very intuitive.
* You can get a binary score for each user: “1” if there is at least a single relevant item in top K or “0” otherwise. Then, you can compute the average hit rate across all users.

![image](https://github.com/user-attachments/assets/267529a6-d2e5-4ce3-99d3-fecb2c58a9e9)

* Note above that the **Hit Rate typically increases with K: there is a higher chance to encounter a relevant item in a longer list.**
* You can also compute multiple Hit Rate measurements at different K, for example, Hit Rate at 3, Hit Rate at 5, and Hit Rate at 10, and track them individually.

## 4. NDCG - Normalized Discounted Cumulative Gain
* This metric considers both the relevance and the position of items in the ranked list.
* Measures the quality of a ranking system, considering the position of relevant items in the ranked list while giving more weight to the items placed higher.
* **NDCG relies on the idea of cumulative gain, which measures the total item relevance in a list.** 
  * To give more credit to the items higher in the ranking, it uses DCG (Discounted Cumulative Gain).
  * DCG introduces a logarithmic discount to give lower weight to items further in the list.


# Behavioral Metrics 

## 1. Diversity
* Evaluates the variety of items recommended to users.
* Assesses how varied the recommended items are for each user. It reflects the breadth of item types or categories to which each user is exposed.
* To compute this metric, you can measure the intra-list diversity by evaluating the average Cosine Distance between pairs of items inside the list. Then, you can average it across all users.   
* Diversity is helpful if you expect users to have a better experience when they receive recommendations that span a diverse range of topics, genres, or characteristics. 
* However, while diversity helps check if a system can show a varied mix of items, it does not consider relevance. 
* You can use this metric with ranking or predictive metrics to get a complete picture.

![image](https://github.com/user-attachments/assets/e719bb7c-43a1-43d6-98e3-e34fd0aa9050)

## 2. Novelty
* Assesses how unique the recommended items are to users. 
* Measures the degree to which the suggested items differ from popular ones.  
* You can compute novelty as the negative logarithm (base 2) of the probability of encountering a given item in a training set. 
  * High novelty corresponds to long-tail items that few users interacted with, and low novelty corresponds to popular items. 
  * Then, you can average the novelty inside the list and across users. 
* Novelty reflects the system's ability to recommend items that are not well-known in the dataset. 
  * It is helpful for scenarios when you expect users to get new and unusual recommendations to stay engaged.
 
![image](https://github.com/user-attachments/assets/de390f06-10ef-496f-a4af-8c1c8041379a)

## 3. Serendipity
* Measures the unexpectedness or pleasant surprise in recommendations. 
* Evaluates the system's ability to suggest items beyond the user's typical preferences or expectations.
* Serendipity is challenging to quantify precisely, but one way to approach it is by considering the dissimilarity (measured via Cosine Distance) between successfully recommended items and a user's historical preferences. 
  * Then, you can average it across users. 
* Serendipity reflects the ability of the system to venture beyond the predictable and offer new recommendations that users enjoy. It promotes exploring diverse and unexpected content, adding an element of delight and discovery.

## 4. Popularity Bias
* Helps assess if there is a bias towards recommending popular items.
* **Popularity bias refers to a phenomenon where the recommendation favors popular items over more diverse or niche ones.**
  * It can lead to a lack of personalization, causing users to see the same widely popular items repeatedly. 
  * This bias may result in a less diverse and engaging user experience.
  * There are different ways to evaluate the popularity of recommendations, for example: 
    * Coverage: the share of all items in the catalog present in recommendations. 
    * Average recommendation popularity (ARP).
    * Average overlap between the items in the lists.
    * Gini index.
   
![image](https://github.com/user-attachments/assets/9f4c00c2-bd5c-4fe6-8c63-db57e12c7bc4)



 # Testing Methodology for Recommendation Systems Offline
* Disclaimer: This is the same standard train-test split method we use in most machine learning models!!

## Method 1: Classical Machine Learning Train/Test Split
* This is the flow of this method:
1. Start with a Full Data set (e.g. movie ratings)
2. Split data into train and test sets (e.g. 70/30)
3. Train set fed into —> Machine learning model —> predictions generated
4. Test set used to measure accuracy, precision, recall, etc.


## Method 2: K-Folds Cross Validation
1. Start with a Full Data set (e.g. movie ratings)
2. Split data into **RANDOM k-folds** (e.g. 3 folds)
3. Each individual fold is run through the ML model
4. For each individual fold we measure accuracy, precision, recall
5. Measure accuracy on the Test Data Set
6. Take average of all results




# How do we come up with an Accuracy metric for Rec Systems?
* These are some well known machine learning metrics often used in supervised learning algorithms such as Linear Regression. 

1. **MAE (mean absolute error)**
 * Most simple or straightforward calculation. 
 * This is the “mean” or “average” error in each prediction rating vs. actual in the system 
 * Goal is the lowest MAE score


2. **RMSE (Root Mean Squared Error)**
 * Penalizes when prediction is way off and penalizes less when prediction are closer.
 * Sum of squares of each error instead of mean. 
 * This will weight larger errors higher than lower errors. 
 * We take square root when done. 
 * Goal is LOW RMSE scores



# Problem —> Accuracy doesnt really tell us ANYTHING in Rec Systems?!
* Where the heck did Accuracy come from anyways?
* This dates back to 2006 and the popular Netflix challenge to come up with a method to evaluate their rec system for their own RMSE score.
* The Prize of the competition was focused on choosing low RMSE scores so it has “stuck” in the industry since then. Bellkor won the prize!
* **However, here's the kicker: this is not ideal and Netflix didnt’ end up using RMSE after all!**



# Top-N Recommender Metrics
1. **Hit Rate**
   * We generate top-n recs for all users in test set. If one of top-n is correct its a “hit”.
   * calculation —> `hit rate = hits / users`
   * Measuring hit rate is difficult because we cant use the same test or cross validation techniques. We aren’t measuring hit rate on the predictions, we are measuring top-n hits which is separate!
   * You could use the training data to measure hit rate but this is not ideal nor is it accurate. 

2. **Leave-one-out-cross-validation**
   * Compute top-n recs for each user in training data and then remove 1 item from each of users train data. 
   * We then test the models ability to predict the top-n item that was left out. 
   * Hit rate with leave-one-out is more user focused, although not ideal because you need. ALOT OF DATA. 

3. **Average Reciprocal Hit Rate (ARHR)**
   * Variation on hit rate
   * Accounts for WHERE in the top-n lists the hits appear.
   * More user focused metric —> tend to get the top hits from the top-n only which biases the results. 
   * Difference in equation: `ARHR = reciprocal rank of hits / users`
   * Here is an example: 
```
Rank         Reciprocal Rank
3                      1/3
2                      1/2
1                       1
```
* Overall ARHR works well if a user has to scroll to see lower ranked items.
* Thus we are penalizing lower ranked items makes sense since the probability of a user finding the lower ranked items is generally lower than the higher ranked items.


4. **Cumulative Hit Rate (cHR)**
   * Throw away hits if predictive rating is below a set threshold. 
   * Concept —> credit not given if we recommend items to a user that we predict they won’t want to see.
   * Here is an example:
```
Hit rank           predicted (or actual) rating
4				5.0
2				3.0
1				5.0
10				2.0
```
   * So above, we would throw away the 2nd and 4th hits as their predicted rating is lower than the threshold of 5.0.


5. **rating hit rate (rHR)**
   * This is the hit rate by predicted rating score
   * Gives a distribution of the ratings
  


# Other Metrics 
1. **Coverage**
   * This is the `% of <user, item>` pairs that can be predicted (possible pairs)
   * Coverage can be at odds with Accuracy —> balance between the two is key!
   * Coverage gives you an idea of when new products/items are “covered”with ratings and patterns. 

2. **Diversity**
   * Measure of how broad a variety of items the system shows to users. 
   * Calculation —> `(1-S)`
   * S = avg similarity between recommendation pairs
   * Similarity scores of most rec systems can be averaged and used to compare the items. S is this metric!
   * You can achieve high diveristy by adding HIGH RANDOMNESS.
   * However, keep in mind HIGH DIVERSITY IS NOT ALWAYS A GOOD THING because high diversity may not be relevant to the user.
   * Thus these are really false positives —> hence why other metrics that look at quality of reccomendations are imporrtant. 




3. **Novelty**
   * This is the “mean popularity rank of recommended items”
   * If you recommend Random things —> it may not mean good results as the user may see things that are NOT RELEVANT to them.
   * Thus, Novelty is not a good thing for most users if they are not familiar with what is being recommended. 
   * **There needs to be a balance between familiarity and new items.**


## “The Long Tail Problem”
* The goal of a rec system should be to surface items to the user in the LONG TAIL. 
* In the plot below we see the # of Sales or popular items sold is on the Y axis, Products are on the X axis.
* It is very common in rec systems to see an Exponential distribution like this. 
   * This is because very often a Large percent of sales are from the most popular items.
   * Thus the purpose of a GOOD Rec system should be to help users discover the long tail items!
* This is why novelty scores are important —> you need a balance between NOVELTY and TRUST in the user to discover the long tail.

![image](https://github.com/user-attachments/assets/478b57de-afc2-4bc7-ba1f-6a26f0a02fe0)
* [image source](https://medium.com/@kyasar.mail/recommender-systems-what-long-tail-tells-91680f10a5b2)
