# Evaluation Metrics for Recommendation Systems
* This repo covers metrics commonly used for recommendation and ranking systems in data science, machine learning, and mathematics.


# Overview
* Recommendations and ranking systems share the goal of returning a list of items sorted by relevance.
* To evaluate recommendations, you must have predictions as user-item pairs, the binary or graded relevance score as the ground truth, and choose the K parameter. 
* The K parameter sets the cutoff point for the top recommendations you evaluate.
* You can use predictive metrics like accuracy or Precision at K or ranking metrics like NDCG, MRR, or MAP at K.  
* To go beyond accuracy, you can use behavioral metrics like serendipity, novelty, or diversity of recommendations. 
* To measure the actual impact, you can track business metrics like sales, click-through rates, or conversions during online evaluations.

# Evaluation Principles

## Input Data
* You need model predictions and ground truths to evaluate recommendation or ranking quality.
* Predictions are a list of ranked items generated by a model. 
* The ground truth is the actual relevance label or score.
 
 
## Relevance
* Relevance reflects the quality of the individual item in a list. 
* It can be…..
  * **Binary score** - (e.g., based on the user action – like clicking, watching, or buying a recommended item) or
  * **Graded score** - (e.g. rating from 1 to 5)
 
## Top-K Parameter
* K parameter is an evaluation cutoff point. 
* It represents the number of top-ranked items to evaluate. 
* For example, you can focus on the quality of top-10 recommendations.


# Metric Types
* Predictive metrics evaluate how accurate a model’s predictions are. 
* Ranking metrics show how well the items in the list are ordered. 
* Behavioral metrics reflect important system properties like diversity of recommendations.

## To evaluate a recommendation or ranking system, you need:
1. **Model predictions** - These include the ranked list of user-item pairs. The complete dataset also contains features that describe users or items. You’ll need them for some of the metrics. 

2. **Ground truth** - You need to know the actual user-item relevance to evaluate the quality of predictions. This might be a binary or graded relevance score. It is often based on the user interactions, such as clicks and conversions. 

3. **The K** - You need to pick the number of the top recommendations to consider. This puts a constraint on evaluations: you will disregard anything that happens after this cutoff point. 

## We can group the recommender or ranking quality metrics into 3 categories:
1. **Predictive metrics**
   * These reflect the “correctness” of recommendations and show how well the system finds relevant items. 

2. **Ranking metrics**
   * These reflect the ranking quality: how well the system can sort the items from more relevant to less relevant.

3. **Behavioral metrics**
   * These metrics reflect specific properties of the system, such as how diverse or novel the recommendations are.
 
## No single metric can capture all aspects of system performance at once. 
  * You can combine several offline metrics for a comprehensive evaluation. 
  * Additionally, you can incorporate online business metrics, user feedback, and subjective evaluations for a more holistic assessment.


# Predictive Metrics

## 1. Precision at K
* This measures the proportion of relevant items among the top K items.

![image](https://github.com/user-attachments/assets/77973096-8ba1-4ac1-bb42-1a420eefccf4)

## 2. Recall at K
* Measures the coverage of relevant items in the top K.

![image](https://github.com/user-attachments/assets/1c19558f-14e7-45ec-8e12-ee7fe73e136f)

## 3. F-score
* The F Beta score is a metric that balances Precision and Recall.
* F Beta score at K combines Precision and Recall metrics into a single value to provide a balanced assessment. 
  * The Beta parameter allows adjusting the importance of Recall relative to Precision. 
* The F Beta score is a good metric when you care about both properties: correctness of predictions and ability to cover as many relevant items as possible with the top-K. 
  * The Beta parameter allows you to customize the priorities.
 
![image](https://github.com/user-attachments/assets/c318e1e1-5127-41ec-8d04-d832a812d1f3)


### Can you limit the ranking evaluation to predictive metrics? 
* Metrics like Precision and Recall are very interpretable and easy to explain to non-technical users.
* They successfully capture the correctness of predictions. 
* Both make them very useful. However, they have limitations. 

**Precision and Recall depend heavily on the total number of relevant items.**
  * Because of this, it might be challenging to compare the performance across different lists.

**In addition, metrics like Precision and Recall are not rank-aware.** 
  * They are indifferent to the position of relevant items inside the top K. 
 
**Consider two lists that both have 5 out of 10 matches.**
  * In the first list, the relevant items are at the very top. 
  * In the second, they are at the very bottom. 
  * The Precision will be the same (50%) as long as the total number of relevant items is.

![image](https://github.com/user-attachments/assets/b01bbe51-5c2e-4442-9955-86538b02b1f1)


# Ranking Metrics
* Ranking metrics help assess the ability to order the items based on their relevance to the user or query. 
* In an ideal system, all relevant items should appear ahead of the less relevant ones. 
* Ranking metrics help measure how far you are from this.
 
## 1. MRR - Mean Reciprocal Rank
* Calculates the average of the reciprocal ranks of the first relevant item.
* To calculate MRR, you take the reciprocal of the rank of the first relevant item and average this value across all queries or users. 
* For example, if the first relevant item appears in the second position, this list's RR (Reciprocal Rank) is 1/2. If the first relevant item takes the third place, then the RR equals 1/3, and so on.

![image](https://github.com/user-attachments/assets/e4930fc6-e3da-47cc-8f85-3d818a30ef3a)

* MRR is an easy-to-understand and intuitive metric. 
  * It is beneficial when the top-ranked item matters: for example, you expect the search engine to return a relevant first result.
* However, the **limitation is that MRR solely focuses on the first relevant item and disregards all the rest.** In case you care about overall ranking, you might need additional metrics.

## 2. MAP - Mean Average Precision
* Measures the average Precision across different Recall levels for a ranked list.
* **Mean Average Precision (MAP) at K** evaluates the average Precision at all relevant ranks within the list of top K recommendations. 
  * This helps get a comprehensive measure of recommendation system performance, accounting for the quality of the ranking.
  * To compute MAP, you first need to calculate the Average Precision (AP) for each list: an average of Precision values at all positions in K with relevant recommendations.
 
![image](https://github.com/user-attachments/assets/5ae9a16a-ebaf-4fb2-991d-74de60123795)

* **MAP helps address the limitations of “classic” Prediction and Recall: it evaluates both the correctness of recommendations and how well the system can sort the relevant items inside the list.**
* Due to the underlying formula, MAP heavily rewards correct recommendations at the top of the list. Otherwise, you will factor the errors at the top in every consecutive Precision computation. 
* MAP is a valuable metric when it is important to get the top predictions right, like in information retrieval. 
* As a downside, this metric might be hard to communicate and does not have an immediate intuitive explanation.

## 3. Hit Rate
* Measures the share of users that get at least one relevant recommendation.
* **Hit Rate at K** calculates the share of users for which at least one relevant item is present in the K. This metric is very intuitive.
* You can get a binary score for each user: “1” if there is at least a single relevant item in top K or “0” otherwise. Then, you can compute the average hit rate across all users.

![image](https://github.com/user-attachments/assets/267529a6-d2e5-4ce3-99d3-fecb2c58a9e9)

* Note above that the **Hit Rate typically increases with K: there is a higher chance to encounter a relevant item in a longer list.**
* You can also compute multiple Hit Rate measurements at different K, for example, Hit Rate at 3, Hit Rate at 5, and Hit Rate at 10, and track them individually.

## 4. NDCG - Normalized Discounted Cumulative Gain
* This metric considers both the relevance and the position of items in the ranked list.
* Measures the quality of a ranking system, considering the position of relevant items in the ranked list while giving more weight to the items placed higher.
* **NDCG relies on the idea of cumulative gain, which measures the total item relevance in a list.** 
  * To give more credit to the items higher in the ranking, it uses DCG (Discounted Cumulative Gain).
  * DCG introduces a logarithmic discount to give lower weight to items further in the list.


# Behavioral Metrics 

## 1. Diversity
* Evaluates the variety of items recommended to users.
* Assesses how varied the recommended items are for each user. It reflects the breadth of item types or categories to which each user is exposed.
* To compute this metric, you can measure the intra-list diversity by evaluating the average Cosine Distance between pairs of items inside the list. Then, you can average it across all users.   
* Diversity is helpful if you expect users to have a better experience when they receive recommendations that span a diverse range of topics, genres, or characteristics. 
* However, while diversity helps check if a system can show a varied mix of items, it does not consider relevance. 
* You can use this metric with ranking or predictive metrics to get a complete picture.

![image](https://github.com/user-attachments/assets/e719bb7c-43a1-43d6-98e3-e34fd0aa9050)

## 2. Novelty
* Assesses how unique the recommended items are to users. 
* Measures the degree to which the suggested items differ from popular ones.  
* You can compute novelty as the negative logarithm (base 2) of the probability of encountering a given item in a training set. 
  * High novelty corresponds to long-tail items that few users interacted with, and low novelty corresponds to popular items. 
  * Then, you can average the novelty inside the list and across users. 
* Novelty reflects the system's ability to recommend items that are not well-known in the dataset. 
  * It is helpful for scenarios when you expect users to get new and unusual recommendations to stay engaged.
 
![image](https://github.com/user-attachments/assets/de390f06-10ef-496f-a4af-8c1c8041379a)

## 3. Serendipity
* Measures the unexpectedness or pleasant surprise in recommendations. 
* Evaluates the system's ability to suggest items beyond the user's typical preferences or expectations.
* Serendipity is challenging to quantify precisely, but one way to approach it is by considering the dissimilarity (measured via Cosine Distance) between successfully recommended items and a user's historical preferences. 
  * Then, you can average it across users. 
* Serendipity reflects the ability of the system to venture beyond the predictable and offer new recommendations that users enjoy. It promotes exploring diverse and unexpected content, adding an element of delight and discovery.

## 4. Popularity Bias
* Helps assess if there is a bias towards recommending popular items.
* **Popularity bias refers to a phenomenon where the recommendation favors popular items over more diverse or niche ones.**
  * It can lead to a lack of personalization, causing users to see the same widely popular items repeatedly. 
  * This bias may result in a less diverse and engaging user experience.
  * There are different ways to evaluate the popularity of recommendations, for example: 
    * Coverage: the share of all items in the catalog present in recommendations. 
    * Average recommendation popularity (ARP).
    * Average overlap between the items in the lists.
    * Gini index.
   
![image](https://github.com/user-attachments/assets/9f4c00c2-bd5c-4fe6-8c63-db57e12c7bc4)

 
